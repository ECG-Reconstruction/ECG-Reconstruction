{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Local machine detected\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import thop\n",
    "from fvcore.nn import FlopCountAnalysis\n",
    "from io import StringIO\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "except ImportError:\n",
    "    logging.info(\"Local machine detected\")\n",
    "    sys.path.append(os.path.realpath(\"..\"))\n",
    "else:\n",
    "    logging.info(\"Colab detected\")\n",
    "    drive.mount(\"/content/drive\")\n",
    "    sys.path.append(\"/content/drive/MyDrive/ecg-reconstruction/src\")\n",
    "\n",
    "from ecg.trainer import Trainer, TrainerConfig\n",
    "from ecg.reconstructor.transformer.transformer import UFormer, NaiveTransformerEncoder\n",
    "from ecg.reconstructor.lstm.lstm import LSTM, CNNLSTM\n",
    "from ecg.reconstructor.cnn.cnn import StackedCNN\n",
    "from ecg.reconstructor.transformer.fastformer import Fastformer, UFastformer, FastformerPlus, FastformerStuff\n",
    "from ecg.reconstructor.unet.unet import UNet\n",
    "from ecg.util.path import resolve_path\n",
    "from ecg.util.tree import deep_merge\n",
    "\n",
    "# dataset_name = \"ptb-xl\"  # \"code15%\"\n",
    "dataset_name = \"code15%\"\n",
    "\n",
    "training_list = [\n",
    "    # StackedCNN,\n",
    "    # LSTM,\n",
    "    # UNet,\n",
    "    # # NaiveTransformerEncoder,\n",
    "    # FastformerPlus,\n",
    "    # CNNLSTM,\n",
    "    # UFormer,\n",
    "    Fastformer,\n",
    "    # UFastformer,\n",
    "    FastformerStuff\n",
    "]\n",
    "\n",
    "base_config: TrainerConfig = {\n",
    "    \"in_leads\": [0, 1, 8],\n",
    "    \"out_leads\": [6, 7, 9, 10, 11],\n",
    "    \"max_epochs\": 32,\n",
    "    \"dataset\": {\n",
    "        \"train\": {\"hdf5_filename\": f\"{dataset_name}/train.hdf5\"},\n",
    "        \"eval\": {\"hdf5_filename\": f\"{dataset_name}/validation.hdf5\"},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model With a New Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Config:\n",
      "accumulate_grad_batches: 8\n",
      "dataloader:\n",
      "    common:\n",
      "        batch_size: 128\n",
      "        num_workers: 6\n",
      "dataset:\n",
      "    common:\n",
      "        feature_scaling: false\n",
      "        filter_args:\n",
      "            N: 3\n",
      "            Wn: !!python/tuple\n",
      "            - 0.5\n",
      "            - 60\n",
      "            btype: bandpass\n",
      "        filter_type: butter\n",
      "        include_filtered_signal: false\n",
      "        include_labels: {}\n",
      "        include_original_signal: false\n",
      "        mean_normalization: true\n",
      "        predicate: null\n",
      "        signal_dtype: float32\n",
      "    eval:\n",
      "        hdf5_filename: code15%/validation.hdf5\n",
      "    train:\n",
      "        hdf5_filename: code15%/train.hdf5\n",
      "in_leads:\n",
      "- 0\n",
      "- 1\n",
      "- 8\n",
      "lr_scheduler:\n",
      "    args:\n",
      "        factor: 0.5986269041609817\n",
      "        patience: 2\n",
      "    type: ReduceLROnPlateau\n",
      "max_epochs: 32\n",
      "optimizer:\n",
      "    args:\n",
      "        betas:\n",
      "        - 0.9091494051250636\n",
      "        - 0.964802538848444\n",
      "        lr: 0.0023091577420568253\n",
      "        weight_decay: 0.0002987140973394854\n",
      "    type: AdamW\n",
      "out_leads:\n",
      "- 6\n",
      "- 7\n",
      "- 9\n",
      "- 10\n",
      "- 11\n",
      "reconstructor:\n",
      "    args:\n",
      "        bert_config_dict:\n",
      "            enable_fp16: false\n",
      "            hidden_act: gelu\n",
      "            hidden_dropout_prob: 0.1\n",
      "            hidden_size: 96\n",
      "            initializer_range: 0.02\n",
      "            intermediate_size: 96\n",
      "            layer_norm_eps: 1.0e-12\n",
      "            max_position_embeddings: 8192\n",
      "            num_attention_heads: 4\n",
      "            num_hidden_layers: 2\n",
      "            pooler_type: weightpooler\n",
      "            type_vocab_size: 2\n",
      "            vocab_size: 100000\n",
      "    type: !!python/name:ecg.reconstructor.transformer.fastformer.Fastformer ''\n",
      "\n",
      "INFO:root:MACs (G): 0.337011\n",
      "INFO:root:Params (M): 0.115125\n",
      "INFO:root:Number f parameters: 901557\n",
      "INFO:root:Config:\n",
      "accumulate_grad_batches: 8\n",
      "dataloader:\n",
      "    common:\n",
      "        batch_size: 128\n",
      "        num_workers: 6\n",
      "dataset:\n",
      "    common:\n",
      "        feature_scaling: false\n",
      "        filter_args:\n",
      "            N: 3\n",
      "            Wn: !!python/tuple\n",
      "            - 0.5\n",
      "            - 60\n",
      "            btype: bandpass\n",
      "        filter_type: butter\n",
      "        include_filtered_signal: false\n",
      "        include_labels: {}\n",
      "        include_original_signal: false\n",
      "        mean_normalization: true\n",
      "        predicate: null\n",
      "        signal_dtype: float32\n",
      "    eval:\n",
      "        hdf5_filename: code15%/validation.hdf5\n",
      "    train:\n",
      "        hdf5_filename: code15%/train.hdf5\n",
      "in_leads:\n",
      "- 0\n",
      "- 1\n",
      "- 8\n",
      "lr_scheduler:\n",
      "    args:\n",
      "        factor: 0.7120742502952482\n",
      "        patience: 3\n",
      "    type: ReduceLROnPlateau\n",
      "max_epochs: 32\n",
      "optimizer:\n",
      "    args:\n",
      "        betas:\n",
      "        - 0.9371313900734933\n",
      "        - 0.9675492630196634\n",
      "        lr: 0.00022889248606975892\n",
      "        weight_decay: 0.0006266790825840408\n",
      "    type: AdamW\n",
      "out_leads:\n",
      "- 6\n",
      "- 7\n",
      "- 9\n",
      "- 10\n",
      "- 11\n",
      "reconstructor:\n",
      "    args:\n",
      "        bert_config_dict:\n",
      "            enable_fp16: false\n",
      "            hidden_act: silu\n",
      "            hidden_dropout_prob: 0.1\n",
      "            hidden_size: 96\n",
      "            initializer_range: 0.02\n",
      "            intermediate_size: 96\n",
      "            layer_norm_eps: 1.0e-12\n",
      "            max_position_embeddings: 8192\n",
      "            num_attention_heads: 4\n",
      "            num_hidden_layers: 4\n",
      "            pooler_type: weightpooler\n",
      "            type_vocab_size: 2\n",
      "            vocab_size: 100000\n",
      "    type: !!python/name:ecg.reconstructor.transformer.fastformer.FastformerStuff ''\n",
      "\n",
      "INFO:root:MACs (G): 2.647562\n",
      "INFO:root:Params (M): 0.897944\n",
      "INFO:root:Number f parameters: 897944\n"
     ]
    }
   ],
   "source": [
    "%%capture captured_output\n",
    "device = \"cpu\"\n",
    "for MODEL_TYPE in training_list:\n",
    "\n",
    "    with open(\n",
    "        resolve_path(\"src/best_configs\") / MODEL_TYPE.__name__ / \"tuned_config.yaml\",\n",
    "        \"r\", encoding=\"utf-8\",\n",
    "    ) as fp:\n",
    "        best_config = yaml.load(fp, Loader=yaml.Loader)\n",
    "\n",
    "    config = deep_merge(best_config, base_config)\n",
    "    config['dataloader']['common']['num_workers'] = 6\n",
    "    config['reconstructor']['type'] = MODEL_TYPE\n",
    "\n",
    "    config_stream = StringIO()\n",
    "    yaml.dump(config, config_stream, yaml.Dumper, indent=4)\n",
    "    logging.info(\"Config:\\n%s\", config_stream.getvalue())\n",
    "    \n",
    "    trainer = Trainer(config)\n",
    "    trainer.reconstructor = trainer.reconstructor.to(device)\n",
    "    total_params = sum(param.numel() for param in trainer.reconstructor.parameters())\n",
    "    device = next(iter(trainer.reconstructor.parameters())).device\n",
    "    dummy_input = torch.from_numpy(trainer.eval_dataset[0][\"input\"][None, ...]).to(device)\n",
    "    with torch.no_grad():\n",
    "        macs, params = thop.profile(trainer.reconstructor, (dummy_input,))\n",
    "        # flops = FlopCountAnalysis(trainer.reconstructor, dummy_input).total()\n",
    "    macs_g = macs / 1e9\n",
    "    params_m = params / 1e6\n",
    "    logging.info(\"MACs (G): %f\", macs_g)\n",
    "    logging.info(\"Params (M): %f\", params_m)\n",
    "    logging.info(\"Number f parameters: %d\", total_params)\n",
    "\n",
    "    # logging.info(\"FLOPs (G): %f\", flops / 1e9)\n",
    "    # trainer.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%capture captured_output\n",
    "\n",
    "MODEL_TYPE = Fastformer\n",
    "\n",
    "with open(\"../checkpoints/FastformerPlus/20230718-2035-code-02/trainer_config.yaml\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    config = yaml.load(fp, Loader=yaml.Loader)\n",
    "\n",
    "trainer = Trainer(config)\n",
    "total_params = sum(param.numel() for param in trainer.reconstructor.parameters())\n",
    "logging.info(\"Number of parameters: %d\", total_params)\n",
    "device = next(iter(trainer.reconstructor.parameters())).device\n",
    "dummy_input = torch.from_numpy(trainer.eval_dataset[0][\"input\"][None, ...]).to(device)\n",
    "with torch.no_grad():\n",
    "    macs, params = thop.profile(trainer.reconstructor, (dummy_input,))\n",
    "macs_g = macs / 1e9\n",
    "params_m = params / 1e6\n",
    "logging.info(\"MACs (G): %f\", macs_g)\n",
    "logging.info(\"Params (M): %f\", params_m)\n",
    "trainer.resume(\n",
    "    checkpoint_dir=\"../checkpoints/FastformerPlus/20230718-2035-code-02\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "x = \"xjjx=1.23.pthxxxdsaf\"\n",
    "# re.findall(r\"=([0-9.]+)\\.pth\", x)[0]current_epoch\n",
    "re.findall(r\"=([0-9.]+)\\.pth\", x)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "This is a simple test. For more complicated analysis, please refer to [`testing notebook`](./src/notebooks/demo_testing_and_visualize.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = UNet\n",
    "\n",
    "checkpoint_dir = resolve_path(\"src/checkpoints\") / MODEL_TYPE.__name__\n",
    "checkpoint_dir /= (checkpoint_dir / \"latest\").read_text().strip()\n",
    "\n",
    "with open(checkpoint_dir / \"trainer_config.yaml\", encoding=\"utf-8\") as config_file:\n",
    "    config = yaml.load(config_file, Loader=yaml.Loader)\n",
    "\n",
    "config[\"dataset\"][\"eval\"][\"hdf5_filename\"] = f\"{dataset_name}/test.hdf5\"\n",
    "\n",
    "trainer = Trainer(config)\n",
    "trainer.load_checkpoint(checkpoint_dir / (checkpoint_dir / \"best\").read_text().strip())\n",
    "trainer.test()\n",
    "logging.info(\"Loss: %f\", trainer.metrics.average_loss.get_average())\n",
    "logging.info(\"RMSE: %f\", trainer.metrics.rmse.get_average())\n",
    "logging.info(\"PearsonR: %f\", trainer.metrics.pearson_r.get_average())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg-reconstruction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
